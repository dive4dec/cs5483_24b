{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9bc3f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "title: Ensemble Methods\n",
    "math:\n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\norm': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "    '\\Gini': '\\operatorname{Gini}'\n",
    "    '\\Info': '\\operatorname{Info}'\n",
    "    '\\Gain': '\\operatorname{Gain}'\n",
    "    '\\GainRatio': '\\operatorname{GainRatio}'\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220cbfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ccca2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Man vs Machine Rematch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e948d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "![](images/RF.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95a251",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Segment Challenge Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e7193a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_man_vs_machine():\n",
    "\n",
    "    # Load the data\n",
    "    rf_data = pd.read_csv(\"RF.csv\")\n",
    "    human_data = pd.read_csv(\"human.csv\")\n",
    "\n",
    "    # Create a combined dataframe with an additional column to distinguish the datasets\n",
    "    rf_data[\"source\"] = \"RF\"\n",
    "    human_data[\"source\"] = \"Human\"\n",
    "    combined_data = pd.concat([rf_data, human_data])\n",
    "\n",
    "    # Exclude data points with missing values\n",
    "    combined_data = combined_data.dropna()\n",
    "\n",
    "    # Function to filter out dominating points\n",
    "    def filter_max_accuracy_points(data):\n",
    "        data = data.sort_values(by=\"depth\")\n",
    "        filtered_data = []\n",
    "\n",
    "        for i, row in data.iterrows():\n",
    "            if not any(\n",
    "                (data[\"depth\"] <= row[\"depth\"]) & (data[\"accuracy\"] > row[\"accuracy\"])\n",
    "            ):\n",
    "                filtered_data.append(row)\n",
    "\n",
    "        return pd.DataFrame(filtered_data)\n",
    "\n",
    "    # Apply the filtering function for each source\n",
    "    max_accuracy_points = (\n",
    "        combined_data.groupby(\"source\")\n",
    "        .apply(filter_max_accuracy_points, include_groups=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Create the scatter plot using go.Scatter\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each source\n",
    "    for source in combined_data[\"source\"].unique():\n",
    "        source_data = combined_data[combined_data[\"source\"] == source]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=source_data[\"depth\"],\n",
    "                y=source_data[\"accuracy\"],\n",
    "                mode=\"markers+text\",\n",
    "                text=source_data[\"name\"],\n",
    "                name=source,\n",
    "                textfont=dict(color=\"rgba(0,0,0,0)\"),  # Make text transparent\n",
    "                marker=dict(size=10),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout with labels and title\n",
    "    fig.update_layout(\n",
    "        title=\"Man vs Machine\", xaxis_title=\"Tree Depth\", yaxis_title=\"Accuracy\"\n",
    "    )\n",
    "\n",
    "    # Add hover information\n",
    "    fig.update_traces(hovertemplate=\"<b>%{text}</b><br>Accuracy: %{y}<br>Depth: %{x}\")\n",
    "\n",
    "    # Add annotations for the points with the highest accuracy\n",
    "    for i, row in max_accuracy_points.iterrows():\n",
    "        fig.add_annotation(\n",
    "            x=row[\"depth\"],\n",
    "            y=row[\"accuracy\"],\n",
    "            text=f\"{row['name']}, {row['accuracy']}\",\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            ax=20,\n",
    "            ay=-30,\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.6)\",\n",
    "            opacity=1,\n",
    "            font=dict(size=10),\n",
    "            hovertext=f\"{row['name']}, {row['accuracy']}\",\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "man_vs_machine_fig = plot_man_vs_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0f967",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "man_vs_machine_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fdec7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Two heads are better than one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa87dc11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- [Bing](https://www.bing.com/translator?from=en&to=zh-Hant&text=Two%20heads%20are%20better%20than%20one)/[Baidu](https://fanyi.baidu.com/#en/zh/Two%20heads%20are%20better%20than%20one)/[Google](https://translate.google.com/#view=home&op=translate&sl=auto&tl=zh-TW&text=Two%20heads%20are%20better%20than%20one) translation.\n",
    "- The story in [Chinese](http://www.youth.com.tw/db/epaper/es001010/eb0758.htm) and its translation to [English](https://translate.google.com/translate?hl=en&sl=auto&tl=en&u=http%3A%2F%2Fwww.youth.com.tw%2Fdb%2Fepaper%2Fes001010%2Feb0758.htm).\n",
    "- Can we combine two poor classifiers into a good classifier?\n",
    "- What is the benefit of doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82ad96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "![](images/ensemble_eg1.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99accd05",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Accuracies of $\\hat{f}_1$ and $\\hat{f}_2$ are both ________%. Are they good?\n",
    "- Can we combine them into a better classifier $\\hat{f}(x) := g(\\hat{f}_1(x), \\hat{f}_2(x))$?\n",
    "- $\\underline{\\kern3em}\\{\\hat{f}_1(x), \\hat{f}_2(x)\\}$ achieves an accuracy of ______________________%.\n",
    "- How does it work in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f9dc2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c02df7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "![](images/arch.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd677fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Base classifiers $\\hat{f}_j$'s are simple but possibly have weak preliminary predictions $\\hat{y}_j$'s.\n",
    "- Combined classifier $\\hat{f}$ uses the combination rule $g$ to merge $\\hat{y}_j$'s into a good final prediction $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66f1a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Architecture for probabilistic classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdfb3d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "![](images/proba.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564353c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Base classifiers $\\hat{f}_j$'s are simple but possibly have weak probability estimates $\\hat{P}_{\\R{Y}|\\RM{X}}^{(j)}(\\cdot \\mid \\M{x})$.\n",
    "- Combined classifier $\\hat{f}$ uses the combination rule $g$ to merge $\\hat{P}_{\\R{Y}|\\RM{X}}^{(j)}(\\cdot \\mid \\M{x})$'s into a good final prediction $\\hat{P}_{\\R{Y}|\\RM{X}}(\\cdot \\mid \\M{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e748a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How to get good performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981d9ed2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Reduce risk by avoiding underfitting and overfitting.\n",
    "- For many loss functions $L$ (0-1 loss, sum of squared error, ...):\n",
    "  $$\n",
    "  \\underbrace{\\E[L(\\R{Y}, \\hat{f}_{\\R{W}}(\\RM{X}))]}_{\\text{Risk}} \\leq \\underbrace{\\E[L(\\R{Y}, \\bar{f}(\\RM{X}))]}_{\\text{Bias}} + \\underbrace{\\E[L(\\bar{f}(\\RM{X}), \\hat{f}_{\\R{W}}(\\RM{X}))]}_{\\text{Variance}}\n",
    "  $$\n",
    "  where\n",
    "- $\\bar{f} := \\M{x} \\mapsto \\E[\\hat{f}_{\\R{W}}(\\M{x})]$ is the **expected predictor** (W is a random variable. Why?).\n",
    "- **Variance** is the dependence of $\\hat{f}_{\\R{W}}(\\RM{X})$ on the data, also known as overfitting/underfitting.\n",
    "- **Bias** is the deviation of $\\hat{f}(\\RM{X})$ from $\\R{Y}$, also known as overfitting/underfitting.\n",
    "- See the [bias-variance trade-off](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52ce5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Bias and variance for probabilistic classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c99fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- For probabilistic classifiers,\n",
    "  $$\n",
    "  \\underbrace{\\E\\left[L(P_{\\R{Y}|\\RM{X}}(\\cdot \\mid \\RM{X}), P_{\\hat{\\R{Y}}|\\RM{X}, \\R{W}}(\\cdot \\mid \\RM{X}, \\R{W}))\\right]}_{\\text{Risk}} \\leq \\underbrace{\\E\\left[L(P_{\\R{Y}|\\RM{X}}(\\cdot \\mid \\RM{X}), P_{\\hat{\\R{Y}}|\\RM{X}}(\\cdot \\mid \\RM{X}))\\right]}_{\\text{Bias}} + \\underbrace{I(\\hat{\\R{Y}}; \\R{W} \\mid \\RM{X})}_{\\text{Variance}}\n",
    "  $$\n",
    "  where\n",
    "    - $f_{\\R{W}}(\\M{x}) := P_{\\hat{\\R{Y}}|\\RM{X}, \\R{W}}(\\cdot \\mid \\M{x}, \\R{W})$ implies\n",
    "      $$\n",
    "      \\bar{f}(\\M{x}) = \\E\\left[P_{\\hat{\\R{Y}}|\\RM{X}, \\R{W}}(\\cdot \\mid \\M{x}, \\R{W})\\right] = P_{\\hat{\\R{Y}}|\\RM{X}}(\\cdot \\mid \\M{x}),\n",
    "      $$\n",
    "      called m______________;\n",
    "    - $P_{\\R{Y}|\\RM{X}}(\\cdot \\mid \\RM{X})$ instead of $\\R{Y}$ is used as the ground truth;\n",
    "    - [Information (or Kullback-Leibler) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) is used as the loss function\n",
    "      $$\n",
    "      L(Q, P) := D_{KL}(P \\parallel Q) := \\int_{\\mathcal{Y}} dP \\log \\frac{dP}{dQ};\n",
    "      $$\n",
    "    - variance becomes the [mutual information](https://en.wikipedia.org/wiki/Mutual_information)\n",
    "      $$\n",
    "      \\E[D_{KL}(P_{\\hat{\\R{Y}}|\\RM{X}, \\R{W}}(\\cdot \\mid \\RM{X}, \\R{W}) \\parallel P_{\\hat{\\R{Y}}|\\RM{X}}(\\cdot \\mid \\RM{X}))] = I(\\hat{\\R{Y}}; \\R{W} \\mid \\RM{X}) \\quad \\because I(\\RM{X}; \\R{W}) = 0.\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d6e25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How to reduce variance and bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8744f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Base classifiers should be diverse, i.e., capture as many different pieces of relevant information as possible to reduce ______.\n",
    "- The combination rule should reduce variance by smoothing out the noise while aggregating relevant information into the final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727ca5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Bagging (Bootstrap Aggregation) Base classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd0ff3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "![](images/bagging.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e467bad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Construct $m$ bootstrap samples.\n",
    "- Construct a base classifier for each bootstrap sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c862c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bagging (Bootstrap Aggregation) Majority voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022cf40c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "![](images/arch.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6022c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "$$\\hat{f}(\\M{x}) := \\arg\\max_{\\hat{y}} \\overbrace{\\left( \\sum_{j} \\mathbb{1} \\left( \\hat{f}_j(\\M{x}) = \\hat{y} \\right) \\right)}^{\\left| \\left\\{ j \\mid \\hat{f}_j(\\M{x}) = \\hat{y} \\right\\} \\right| = }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868685cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e993c9fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "![](images/bagging_eg1.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c365ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/bagging_eg2.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64aae69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Accuracy = _________________________%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1427c49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Is it always good to follow the majority?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1e5e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](images/bagging_eg3.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b74127c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Accuracy = _________________________%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe113b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- It is beneficial to return 0 more often because _________________________.\n",
    "- How to do this in general?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e3142",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Sum rule and threshold moving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51841b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- $\\hat{f}(\\M{x}) = 1$ iff \n",
    "  $$\n",
    "  \\frac{1}{2} \\left[ \\hat{f}_1(\\M{x}) + \\hat{f}_2(\\M{x}) \\right] > \\underline{\\hspace{2cm}}\n",
    "  $$\n",
    "\n",
    "- Binary classification: Choose $\\hat{f}(\\M{x}) = 1$ iff \n",
    "  $$\n",
    "  \\frac{1}{m} \\sum_{t} \\hat{f}_t(\\M{x}) > \\gamma\n",
    "  $$\n",
    "  for some chosen threshold $\\gamma$.\n",
    "\n",
    "- What about multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ba928",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bagging (Bootstrap Aggregation) Average of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd24c03",
   "metadata": {},
   "source": [
    "![](images/proba.dio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ee25b",
   "metadata": {},
   "source": [
    "$$\\hat{f}(\\M{x}) := \\frac{1}{m} \\sum_{t} \\hat{f}_t(\\M{x}) > \\gamma$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b51ce0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Other techniques to diversify base classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69d584",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- **Random forest**: Bagging with modified decision tree induction\n",
    "    - **Forest-RI**: For each split, consider random i___________________ s___________________ where only $F$ randomly chosen features are considered.\n",
    "    - **Forest-RC**: For each split, consider $F$ random l___________________ c___________________ of $L$ randomly chosen features.\n",
    "- **Voting** (weka.classifier.meta.vote) and **Stacking** (weka.classifier.meta.stacking): \n",
    "    - Use different classification algorithms.\n",
    "- **Adaptive boosting (Adaboost)**:\n",
    "    - Each base classifier tries to _______________________________ made by previous base classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1546b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Other techniques to combine decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d85e22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- **Random forest**: Bagging with modified decision tree induction\n",
    "    - **Majority voting**\n",
    "    - **Average of probabilities**\n",
    "- **Voting**\n",
    "    - **Majority voting or median**\n",
    "    - **Average/product/minimum/maximum probabilities**\n",
    "- **Stacking**: Use a meta classifier.\n",
    "    - **Adaptive boosting (Adaboost)**: 2003 [GÃ¶del Prize](https://en.wikipedia.org/wiki/G%C3%B6del_Prize) winner\n",
    "    - **Weighted majority voting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f5d1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### What is Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57c650",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- An ensemble method that learns from mistakes:\n",
    "    - Combined classifier: Majority voting but with more weight on more accurate base classifier.\n",
    "      $$\n",
    "      \\hat{f}(\\M{x}) := \\arg\\max_{\\hat{y}} \\sum_{t} w_t \\cdot \\mathbb{1}((\\hat{f}_t)(\\M{x}) = \\hat{y})\n",
    "      $$\n",
    "      where \n",
    "      $$\n",
    "      w_t := \\frac{1}{2} \\ln \\left( \\frac{1 - \\text{error}(\\hat{f}_t)}{\\text{error}(\\hat{f}_t)} \\right)\n",
    "      $$\n",
    "      is the amount of say of $\\hat{f}_t$ and $\\text{error}(\\hat{f}_t)$ is the error rate w.r.t. $D_t$. (See the precise formula below.)\n",
    "    - Base classifiers: Train $\\hat{f}_t$ sequentially in $t$ on $D_t$ obtained by Bagging $(\\M{x}_i, \\R{Y}_i) \\in D$ with\n",
    "      $$\n",
    "      p_i^{(t)} := \\frac{p_i^{(t-1)}}{Z_t} \\times \\begin{cases} \n",
    "      e^{w_{t-1}}, & \\hat{f}_{t-1}(\\M{x}_i) \\neq \\R{Y}_i \\text{ (incorrectly classified example)} \\\\\n",
    "      e^{-w_{t-1}}, & \\text{otherwise (correctly classified example)}\n",
    "      \\end{cases}\n",
    "      $$\n",
    "      starting with $p_i^{(1)} := \\frac{1}{|D|}$ and with $Z_t > 0$ chosen so that $\\sum_{i} p_i^{(t)} = 1$.\n",
    "    - Compute the error rate\n",
    "      $$\n",
    "      \\text{error}(\\hat{f}_t) := \\sum_{i} p_i^{(t)} \\cdot \\mathbb{1}((\\hat{f}_t)(\\M{x}_i) \\neq \\R{Y}_i)\n",
    "      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a851efe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Machine vs Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde508d2",
   "metadata": {},
   "source": [
    "![](images/ADB.dio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9d33e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_machine_vs_machine():\n",
    "\n",
    "    # Load the data\n",
    "    rf_data = pd.read_csv(\"RF.csv\")\n",
    "    adb_data = pd.read_csv(\"ADB.csv\")\n",
    "\n",
    "    # Create a combined dataframe with an additional column to distinguish the datasets\n",
    "    rf_data[\"source\"] = \"RF\"\n",
    "    adb_data[\"source\"] = \"ADB\"\n",
    "    combined_data = pd.concat([rf_data, adb_data])\n",
    "\n",
    "    # Exclude data points with missing values\n",
    "    combined_data = combined_data.dropna()\n",
    "\n",
    "    # Function to filter out dominating points\n",
    "    def filter_max_accuracy_points(data):\n",
    "        data = data.sort_values(by=\"depth\")\n",
    "        filtered_data = []\n",
    "\n",
    "        for i, row in data.iterrows():\n",
    "            if not any(\n",
    "                (data[\"depth\"] <= row[\"depth\"]) & (data[\"accuracy\"] > row[\"accuracy\"])\n",
    "            ):\n",
    "                filtered_data.append(row)\n",
    "\n",
    "        return pd.DataFrame(filtered_data)\n",
    "\n",
    "    # Apply the filtering function for each source\n",
    "    max_accuracy_points = (\n",
    "        combined_data.groupby(\"source\")\n",
    "        .apply(filter_max_accuracy_points, include_groups=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Create the scatter plot using go.Scatter\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add traces for each source\n",
    "    for source in combined_data[\"source\"].unique():\n",
    "        source_data = combined_data[combined_data[\"source\"] == source]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=source_data[\"depth\"],\n",
    "                y=source_data[\"accuracy\"],\n",
    "                mode=\"markers+text\",\n",
    "                text=source_data[\"name\"],\n",
    "                name=source,\n",
    "                textfont=dict(color=\"rgba(0,0,0,0)\"),  # Make text transparent\n",
    "                marker=dict(size=10),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Update layout with labels and title\n",
    "    fig.update_layout(\n",
    "        title=\"Machine vs Machine\", xaxis_title=\"Tree Depth\", yaxis_title=\"Accuracy\"\n",
    "    )\n",
    "\n",
    "    # Add hover information\n",
    "    fig.update_traces(hovertemplate=\"<b>%{text}</b><br>Accuracy: %{y}<br>Depth: %{x}\")\n",
    "\n",
    "    # Add annotations for the points with the highest accuracy\n",
    "    for i, row in max_accuracy_points.iterrows():\n",
    "        fig.add_annotation(\n",
    "            x=row[\"depth\"],\n",
    "            y=row[\"accuracy\"],\n",
    "            text=f\"{row['name']}, {row['accuracy']}\",\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            ax=20,\n",
    "            ay=-30,\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.6)\",\n",
    "            opacity=1,\n",
    "            font=dict(size=10),\n",
    "            hovertext=f\"{row['name']}, {row['accuracy']}\",\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "machine_vs_machine_fig = plot_machine_vs_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9c017",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "machine_vs_machine_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b7287",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c439122",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- Techniques to improve classification accuracy\n",
    "- [Witten11] Chapter 8\n",
    "- Optional:\n",
    "  - Breiman, L. (1996). [\"Bagging predictors.\"](https://doi.org/10.1007%2FBF00058655) Machine learning, 24(2), 123-140.\n",
    "  - Breiman, L. (2001). [\"Random forests.\"](https://doi.org/10.1023%2FA%3A1010933404324) Machine learning, 45(1), 5-32.\n",
    "  - Freund Y, Schapire R, Abe N. [\"A short introduction to boosting.\"](http://www.yorku.ca/gisweb/eats4400/boost.pdf) Journal-Japanese Society For Artificial Intelligence. 1999 Sep 1;14(771-780):1612.\n",
    "  - Zhu, H. Zou, S. Rosset, T. Hastie, [\"Multi-class AdaBoost\"](https://www.intlpress.com/site/pub/files/_fulltext/journals/sii/2009/0002/0003/SII-2009-0002-0003-a008.pdf), 2009."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
