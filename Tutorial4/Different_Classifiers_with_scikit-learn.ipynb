{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5062d3cc",
   "metadata": {},
   "source": [
    "---\n",
    "title: Different Classifiers with scikit-learn\n",
    "math: \n",
    "    '\\abs': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\norm': '\\left\\lvert #1 \\right\\rvert' \n",
    "    '\\Set': '\\left\\{ #1 \\right\\}'\n",
    "    '\\mc': '\\mathcal{#1}'\n",
    "    '\\M': '\\boldsymbol{#1}'\n",
    "    '\\R': '\\mathsf{#1}'\n",
    "    '\\RM': '\\boldsymbol{\\mathsf{#1}}'\n",
    "    '\\op': '\\operatorname{#1}'\n",
    "    '\\E': '\\op{E}'\n",
    "    '\\d': '\\mathrm{\\mathstrut d}'\n",
    "    '\\Gini': '\\operatorname{Gini}'\n",
    "    '\\Info': '\\operatorname{Info}'\n",
    "    '\\Gain': '\\operatorname{Gain}'\n",
    "    '\\GainRatio': '\\operatorname{GainRatio}'\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f71d54",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "from sklearn import datasets, neighbors, preprocessing, tree\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from util import plot_decision_regions\n",
    "\n",
    "%matplotlib widget\n",
    "if not os.getenv(\n",
    "    \"NBGRADER_EXECUTION\"\n",
    "):\n",
    "    %load_ext jupyter_ai\n",
    "    %ai update chatgpt dive:chat\n",
    "    # %ai update chatgpt dive-azure:gpt4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f076751",
   "metadata": {},
   "source": [
    "## Normalization of Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5dcdb8",
   "metadata": {},
   "source": [
    "For this notebook, we consider the binary classification problem on the [breast cancer dataset](https://doi.org/10.24432/C5DW2B) in [(Street el al. 2013)](https://doi.org/10.1117/12.148698):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8ea3a",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# load the dataset from sklearn\n",
    "dataset = datasets.load_breast_cancer()\n",
    "\n",
    "# create a DataFrame to help further analysis\n",
    "df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "df[\"target\"] = dataset.target\n",
    "df.target = df.target.astype(\"category\").cat.rename_categories(\n",
    "    dict(zip(range(3), dataset.target_names))\n",
    ")\n",
    "df  # display an overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29553f78",
   "metadata": {},
   "source": [
    "The goal is to train a classifier to diagnose whether a breast mass is malignant or benign. The target class distribution is shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66173030",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(num=1, clear=True)\n",
    "display(df.target.value_counts())\n",
    "df.target.value_counts().plot(kind=\"bar\", title=\"counts of different classes\", rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18d567",
   "metadata": {},
   "source": [
    "The input features are characteristics of cell images obtained by the [fine needle analysis (FNA)](https://en.wikipedia.org/wiki/Fine-needle_aspiration). See if LLM may help give more information on the input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c29267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt -f text\n",
    "Explain each input feature of the fine needle analysis in one line:\n",
    "--\n",
    "{df.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bfef50",
   "metadata": {},
   "source": [
    "To gain insights into how each input feature varies with different class values, we can examine the statistics of each input feature grouped by the class values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e5e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_statistics(df, **kwargs): \n",
    "    grps = df.groupby(\"target\", observed=False)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(grps), sharey=True, clear=True, figsize=(10, 9), layout=\"constrained\", squeeze=False, **kwargs)\n",
    "    for grp, ax in zip(df.groupby(\"target\", observed=False), axes[0]):\n",
    "        grp[1].boxplot(rot=90, fontsize=7, ax=ax).set_title(grp[0])\n",
    "    plt.show()\n",
    "\n",
    "show_feature_statistics(df, num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca39b5f",
   "metadata": {},
   "source": [
    "From the above plots, it can be observed that the attributes `mean area` and `worst area` have much larger ranges than other features have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966938d8",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:1\n",
    "\n",
    "Does a larger range make a feature better? Why or why not?\n",
    " \n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f488b6f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c4db5e8ba428be653c58f92819cf9c0",
     "grade": true,
     "grade_id": "range",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdbead",
   "metadata": {},
   "source": [
    "### Min-max Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6087757",
   "metadata": {},
   "source": [
    "We can normalize a numeric feature $\\R{Z}$ to the unit interval as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\R{Z}':= \\frac{\\R{Z}}{b - a}\n",
    "\\end{align}\n",
    "$$ (min-max)\n",
    "\n",
    "where $a$ and $b$ are respectively the minimum and maximum possible values of $\\R{Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7747ca",
   "metadata": {},
   "source": [
    "In practice, $a$ and $b$ may be unknown as the distribution of $\\R{Z}$ is unknown. We perform the normalization on the samples: The min-max normalization of the sequence (in $i$) of $z_i$ is the sequence of\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z'_i := \\frac{z_i - \\min_j z_j}{\\max_j z_j - \\min_j z_j},\n",
    "\\end{align}\n",
    "$$ (min-max-sample)\n",
    "\n",
    "where $\\min_j z_j$ and $\\max_j z_j$ are respectively the minimum and maximum sample values. It follows that $0\\leq z'_i \\leq 1$ and the equalities hold with equality for some indices $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e10d4",
   "metadata": {},
   "source": [
    "An implementation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d0e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normalize(df, suffix=\" (min-max normalized)\"):\n",
    "    \"\"\"\n",
    "    Min-max normalize numerical attributes of the input DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Input DataFrame to be min-max normalized. May contain both numeric and \n",
    "        categorical attributes.\n",
    "    suffix : str, optional\n",
    "        Suffix to append to the names of normalized attributes (default is \n",
    "        \" (min-max normalized)\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        A copy of the input DataFrame with its numeric attributes replaced by their \n",
    "        min-max normalization. The normalized features are renamed with the suffix \n",
    "        appended to the end of their original names.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # avoid overwriting the original dataframe\n",
    "    min_values = df.select_dtypes(include=\"number\").min()  # Skip categorical features\n",
    "    max_values = df[min_values.index].max()\n",
    "\n",
    "    # min-max normalize\n",
    "    df[min_values.index] = (df[min_values.index] - min_values) / (\n",
    "        max_values - min_values\n",
    "    )\n",
    "\n",
    "    # rename normalized features\n",
    "    df.rename(columns={c: c + suffix for c in min_values.index}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eff558",
   "metadata": {},
   "source": [
    "Renaming normalized features helps differentiate them from the original ones. The statistics of the normalized features are given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax_normalized = minmax_normalize(df)\n",
    "assert df_minmax_normalized.target.to_numpy().base is df.target.to_numpy().base\n",
    "\n",
    "show_feature_statistics(df_minmax_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efb297",
   "metadata": {},
   "source": [
    "We can see how instances of different classes differ in different input features other than `mean area` and `worst area`. In particular, both `mean-concavity` and `worst-concavity` are substantially higher for malignant examples than for benign examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd29a8",
   "metadata": {},
   "source": [
    "### Standard Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85ce69",
   "metadata": {},
   "source": [
    "Min-max normalization is not appropriate for features with unbounded support where $b-a=\\infty$ in {eq}`min-max`. The normalization factor $\\max_j z_j - \\min_j z_j$ in {eq}`min-max-sample` for i.i.d. samples will approach $\\infty$ as the number of samples goes to infinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2014bfae",
   "metadata": {},
   "source": [
    "Let us inspect the distribution of each feature using [`displot`](https://seaborn.pydata.org/generated/seaborn.displot.html) provided by the package [`seaborn`](https://seaborn.pydata.org), which is imported with\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(\n",
    "    feature=dataset.feature_names, kernel_density_estimation=True, group_by_class=False\n",
    ")\n",
    "def plot_distribution(feature, kernel_density_estimation, group_by_class):\n",
    "    grps = df.groupby(\"target\", observed=False) if group_by_class else [('', df)]\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(grps), clear=True, figsize=(10, 5), layout=\"constrained\", num=4, squeeze=False, sharey=True)\n",
    "    for grp, ax in zip(grps, axes[0]):\n",
    "        sns.histplot(data=grp[1], x=feature, kde=kernel_density_estimation, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ea147",
   "metadata": {},
   "source": [
    "Play with the above widgets to check if the input features have unbounded support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa13071",
   "metadata": {},
   "source": [
    "For a feature $\\R{Z}$ with unbounded support, one may use the $z$-score/standard normalization instead:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\R{Z}' := \\frac{\\R{Z} - E[\\R{Z}]}{\\sqrt{\\operatorname{Var}(\\R{Z})}}.\n",
    "\\end{align}\n",
    "$$ (standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e517a",
   "metadata": {},
   "source": [
    "Since the distribution of $\\R{Z}$ is unknown, we normalize the sequence of i.i.d. samples $z_i$ using its sample mean $\\mu$ and standard deviation $\\sigma$ to the sequence of\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "z'_i := \\frac{z_i - \\mu}{\\sigma}. \n",
    "\\end{align}\n",
    "$$ (standard-sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe00c64",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:2\n",
    " Complete the function `standard_normalize` as follows:\n",
    "\n",
    "- Return a new copy of the input `DataFrame` `df` but with all its numeric attributes standard normalized. \n",
    "- You may use the methods `mean` and `std`.\n",
    "- Rename the normalized features by appending `suffix` to their names.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59702e2a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed43c0750220b0be41534cd22f42b778",
     "grade": false,
     "grade_id": "normalize-attributes",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "def standard_normalize(df, suffix=\" (standard normalized)\"):\n",
    "    \"\"\"Returns a DataFrame with numerical attributes of the input DataFrame\n",
    "    standard normalized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: DataFrame\n",
    "        Input to be standard normalized. May contain both numeric\n",
    "        and categorical attributes.\n",
    "    suffix: string\n",
    "        Suffix to append to the names of normalized attributes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame:\n",
    "        A new copy of df that retains the categorical attributes but with the\n",
    "        numeric attributes replaced by their standard normalization.\n",
    "        The normalized features are renamed with the suffix appended to the end\n",
    "        of their original names.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "df_standard_normalized = standard_normalize(df)\n",
    "show_feature_statistics(df_standard_normalized, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b9930e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04b496bd07b3c05177f5daf0ed39b658",
     "grade": true,
     "grade_id": "test-standard-normalization",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# tests\n",
    "assert np.isclose(\n",
    "    df_standard_normalized.select_dtypes(include=\"number\").mean(), 0\n",
    ").all()\n",
    "assert np.isclose(df_standard_normalized.select_dtypes(include=\"number\").std(), 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8d5fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daff4ae9be4ab1b656bb9d186062b798",
     "grade": true,
     "grade_id": "htest-standard-normalization",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b4651",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fd94f",
   "metadata": {},
   "source": [
    "To create a $k$-nearest-neighbor ($k$-NN) classifier, we can use `sklearn.neighbors.KNeighborsClassifier`. The following fits a $1$-NN classifier to the entire dataset and returns its training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512aa652",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = df[dataset.feature_names], df.target\n",
    "kNN1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "kNN1.fit(X, Y)\n",
    "\n",
    "print(\"Training accuracy: {:0.3g}\".format(kNN1.score(X, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c7179",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:3\n",
    " Why is the training accuracy for $1$-NN $100\\%$? Explain according to how 1-NN works.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a76c44",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4af86ffab706c5d80300de98c370c2f",
     "grade": true,
     "grade_id": "1-NN",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b7ef2",
   "metadata": {},
   "source": [
    "To avoid overly-optimistic performance estimates, the following uses 10-fold cross validation to compute the accuracies of 1-NN trained on datasets with and without normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "\n",
    "dfs = {\"None\": df, \"Min-max\": df_minmax_normalized}\n",
    "\n",
    "acc = pd.DataFrame(columns=dfs.keys())\n",
    "for norm in dfs:\n",
    "    acc[norm] = cross_val_score(\n",
    "        kNN1,\n",
    "        dfs[norm].loc[:, lambda df: ~df.columns.isin([\"target\"])],\n",
    "        # not [dataset.feature_names] since normalized features are renamed\n",
    "        dfs[norm][\"target\"],\n",
    "        cv=cv,\n",
    "    )\n",
    "\n",
    "acc.agg([\"mean\", \"std\"]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee23f63",
   "metadata": {},
   "source": [
    "The accuracies show that normalization improves the performance of 1-NN. More precisely, the accuracy improvement of $\\sim 5\\%$ appears statistically insignificant because it is at least twice the standard deviations of $\\sim 2\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5e98a",
   "metadata": {},
   "source": [
    "::::{seealso} How to compare performance?\n",
    "\n",
    "The proper way to compare performance should consider statistical significance, such as the corrected resampled paired $t$-test [(Nadeau and Bengio, 2003)](https://doi.org/10.1023/A:1024068626366), which is [implemented in Weka](https://weka.sourceforge.io/doc.dev/weka/experiment/PairedCorrectedTTester.html). For an implementation in Python, see the sklearn documentation [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_stats.html#comparing-two-models-frequentist-approach). \n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt -f text\n",
    "Explain and introduce the formulae for the paired t-test and the \n",
    "corrected resampled paired t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt -f text\n",
    "How to reduce improve the statistical significance when comparing the \n",
    "performance of two learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8fda59",
   "metadata": {},
   "source": [
    "### Data Leak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca74bb",
   "metadata": {},
   "source": [
    "The accuracies computed for the normalizations above suffer from a subtle issue that renders them overly optimistic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b13032",
   "metadata": {},
   "source": [
    "::::{caution}\n",
    "\n",
    "Since the normalization factors for cross validation were calculated from the entire dataset, the test data for each cross-validation fold may not be independent of the remaining normalized data for training the classifier. This subtle data leak may cause the performance estimate to be overly-optimistic.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba07bf17",
   "metadata": {},
   "source": [
    "This issue can be resolved by computing the normalization factors from the training set instead of the entire dataset. To do so, we will create a pipeline using the following:\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "```\n",
    "\n",
    "- Like the filtered classifier in Weka, `sklearn.pipeline` provides the function `make_pipeline` to combine a filter with a classifier.\n",
    "- `sklearn.preprocessing` provides different filters for preprocessing features, , e.g., `StandardScaler` and `MinMaxScaler` for \n",
    "\n",
    "Creating a pipeline is especially useful for cross validation, where the normalization factors must be recomputed for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a60415",
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN1_standard_normalized = make_pipeline(preprocessing.StandardScaler(), kNN1)\n",
    "acc[\"Standard\"] = cross_val_score(kNN1_standard_normalized, X, Y, cv=cv)\n",
    "acc[\"Standard\"].agg([\"mean\", \"std\"]).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4155c7",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:4\n",
    " Similar to the above cell, correct the accuracies in `acc['Min-max']` to use `preprocessing.MinMaxScaler` as part of a pipeline for the 1-NN classifier.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb45ff",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6382823e1a4f36b3ab8c320ad6b6921a",
     "grade": false,
     "grade_id": "min-max",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError\n",
    "acc[\"Min-max\"].agg([\"mean\", \"std\"]).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0dec7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02df299a4ac68648a291b58a2764dd51",
     "grade": true,
     "grade_id": "test-min-max",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# hidden tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt -f text\n",
    "Is there a data leak if I manually preprocess the data to maximize the \n",
    "classification accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afada015",
   "metadata": {},
   "source": [
    "### Decision Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bceace0",
   "metadata": {},
   "source": [
    "Since `sklearn` does not provide any function to plot the decision regions of a classifier, we provide the function `plot_decision_regions` in a module `util` defined in [`util.py`](util.py) of the current directory:\n",
    "\n",
    "```python\n",
    "from util import plot_decision_regions\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5246a",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "plot_decision_regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf0471",
   "metadata": {},
   "source": [
    "The following plots the decision region for a selected pair of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\n",
    "    \"NBGRADER_EXECUTION\"\n",
    "):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, clear=True, figsize=(10, 10), layout=\"constrained\", num=6, sharey=True)\n",
    "    @interact(\n",
    "        normalization=[\"None\", \"Min-max\", \"Standard\"],\n",
    "        feature1=dataset.feature_names,\n",
    "        feature2=dataset.feature_names,\n",
    "        k=widgets.IntSlider(1, 1, 5, continuous_update=False),\n",
    "        resolution=widgets.IntSlider(1, 1, 4, continuous_update=False),\n",
    "    )\n",
    "    def decision_regions_kNN(\n",
    "        normalization,\n",
    "        feature1=dataset.feature_names[0],\n",
    "        feature2=dataset.feature_names[1],\n",
    "        k=1,\n",
    "        resolution=1,\n",
    "    ):\n",
    "        scaler = {\n",
    "            \"Min-max\": preprocessing.MinMaxScaler,\n",
    "            \"Standard\": preprocessing.StandardScaler,\n",
    "        }\n",
    "        kNN = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "        if normalization != \"None\":\n",
    "            kNN = make_pipeline(scaler[normalization](), kNN)\n",
    "        kNN.fit(df[[feature1, feature2]].to_numpy(), df.target.to_numpy())\n",
    "        ax.clear()\n",
    "        plot_decision_regions(\n",
    "            df[[feature1, feature2]], df.target, kNN, N=resolution * 100,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(\"Decision region for {}-NN\".format(k))\n",
    "        ax.set_xlabel(feature1)\n",
    "        ax.set_ylabel(feature2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e740e",
   "metadata": {},
   "source": [
    "Interact with the widgets to: \n",
    "\n",
    "- Learn the effect on the decision regions/boundaries with different normalizations and choices of $k$.\n",
    "- Learn to draw the decision boundaries for $1$-NN with min-max normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac0019",
   "metadata": {},
   "source": [
    "::::{exercise}\n",
    ":label: ex:5\n",
    " Complete the following code to plot the decision regions for decision trees. Afterward, explain whether the decision regions change for different normalizations.\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee0fa5",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1dad4ee342a9f777455d5b78f004fe6d",
     "grade": false,
     "grade_id": "DT-region",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\n",
    "    \"NBGRADER_EXECUTION\"\n",
    "):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, clear=True, figsize=(10, 10), layout=\"constrained\", num=7, sharey=True)\n",
    "    @interact(\n",
    "        normalization=[\"None\", \"Min-max\", \"Standard\"],\n",
    "        feature1=dataset.feature_names,\n",
    "        feature2=dataset.feature_names,\n",
    "        resolution=widgets.IntSlider(1, 1, 4, continuous_update=False),\n",
    "    )\n",
    "    def decision_regions_kNN(\n",
    "        normalization,\n",
    "        feature1=dataset.feature_names[0],\n",
    "        feature2=dataset.feature_names[1],\n",
    "        resolution=1,\n",
    "    ):\n",
    "        scaler = {\n",
    "            \"Min-max\": preprocessing.MinMaxScaler,\n",
    "            \"Standard\": preprocessing.StandardScaler,\n",
    "        }\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        ax.clear()\n",
    "        plot_decision_regions(\n",
    "            df[[feature1, feature2]], df.target, DT, N=resolution * 100,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.set_title(\"Decision region for Decision Tree\")\n",
    "        ax.set_xlabel(feature1)\n",
    "        ax.set_ylabel(feature2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19729a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f006f32a0176204e7bba0f6eb5a48ff3",
     "grade": true,
     "grade_id": "DT-decision-regions",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61eff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ai chatgpt -f text\n",
    "Is it possible to visualize the decision region of a classifier on \n",
    "high-dimensional data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
